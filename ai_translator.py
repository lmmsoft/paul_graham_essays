#!/usr/bin/env python3
"""
ä½¿ç”¨SiliconFlowçš„DeepSeek-V3æ¨¡å‹ç¿»è¯‘Paul Grahamæ–‡ç« 
"""

import concurrent.futures
import hashlib
import json
import os
import re
import time
from datetime import datetime
from pathlib import Path
from threading import Lock

import requests
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class DeepSeekTranslator:
    def __init__(self, api_key, cache_dir="translation_cache", max_concurrent=8):
        self.api_key = api_key
        self.base_url = "https://api.siliconflow.cn/v1/chat/completions"
        self.model = "deepseek-ai/DeepSeek-V3"
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # ç¼“å­˜è®¾ç½®
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # å¹¶å‘æ§åˆ¶ - é’ˆå¯¹ä½ çš„APIé™åˆ¶ä¼˜åŒ– (RPM:1000, TPM:10000)
        self.max_concurrent = min(max_concurrent, 8)  # ä¿å®ˆçš„å¹¶å‘æ•°
        self.request_interval = 0.1  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
        self.last_request_time = 0
        self.request_lock = Lock()
        
        # ç»Ÿè®¡
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'api_calls': 0,
            'failed_requests': 0
        }
        self.stats_lock = Lock()
    
    def get_cache_key(self, text):
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.sha256(text.encode('utf-8')).hexdigest()
    
    def get_cached_translation(self, text):
        """è·å–ç¼“å­˜çš„ç¿»è¯‘"""
        cache_key = self.get_cache_key(text)
        cache_file = self.cache_dir / f"{cache_key}.json"
        
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    with self.stats_lock:
                        self.stats['cache_hits'] += 1
                    return data['translation']
            except (json.JSONDecodeError, KeyError):
                pass
        return None
    
    def save_translation_cache(self, text, translation):
        """ä¿å­˜ç¿»è¯‘åˆ°ç¼“å­˜"""
        cache_key = self.get_cache_key(text)
        cache_file = self.cache_dir / f"{cache_key}.json"
        
        cache_data = {
            'original': text[:200] + "..." if len(text) > 200 else text,
            'translation': translation,
            'timestamp': datetime.now().isoformat(),
            'model': self.model
        }
        
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

    def translate_text(self, text, context=""):
        """ç¿»è¯‘æ–‡æœ¬ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        with self.stats_lock:
            self.stats['total_requests'] += 1
        
        # æ£€æŸ¥ç¼“å­˜
        cached = self.get_cached_translation(text)
        if cached:
            return cached
        
        prompt = f"""è¯·å°†ä»¥ä¸‹è‹±æ–‡æ–‡ç« ç¿»è¯‘æˆä¸­æ–‡ï¼Œç›´æ¥è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è¯´æ˜æˆ–æ³¨é‡Šã€‚

{context}

åŸæ–‡ï¼š
{text}

ä¸­æ–‡ç¿»è¯‘ï¼š"""

        data = {
            "model": self.model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.3,
            "max_tokens": 4000
        }

        try:
            with self.stats_lock:
                self.stats['api_calls'] += 1
            
            response = requests.post(self.base_url, headers=self.headers, json=data, timeout=60)
            response.raise_for_status()
            result = response.json()

            if 'choices' in result and len(result['choices']) > 0:
                translation = result['choices'][0]['message']['content'].strip()
                # ä¿å­˜åˆ°ç¼“å­˜
                self.save_translation_cache(text, translation)
                return translation
            else:
                print(f"APIå“åº”æ ¼å¼é”™è¯¯: {result}")
                with self.stats_lock:
                    self.stats['failed_requests'] += 1
                return None

        except requests.exceptions.RequestException as e:
            print(f"APIè¯·æ±‚å¤±è´¥: {e}")
            with self.stats_lock:
                self.stats['failed_requests'] += 1
            return None
        except json.JSONDecodeError as e:
            print(f"JSONè§£æå¤±è´¥: {e}")
            with self.stats_lock:
                self.stats['failed_requests'] += 1
            return None

    def split_article(self, content, max_chunk_size=3000):
        """å°†æ–‡ç« åˆ†å‰²æˆè¾ƒå°çš„å—è¿›è¡Œç¿»è¯‘"""
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        chunks = []
        current_chunk = ""

        for paragraph in paragraphs:
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½ä¸ä¼šè¶…è¿‡é™åˆ¶
            if len(current_chunk) + len(paragraph) + 2 < max_chunk_size:
                if current_chunk:
                    current_chunk += '\n\n' + paragraph
                else:
                    current_chunk = paragraph
            else:
                # ä¿å­˜å½“å‰å—ï¼Œå¼€å§‹æ–°å—
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = paragraph

        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunks.append(current_chunk)

        return chunks
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“Š ç¿»è¯‘ç»Ÿè®¡:")
        print(f"  æ€»è¯·æ±‚æ•°: {self.stats['total_requests']}")
        print(f"  ç¼“å­˜å‘½ä¸­: {self.stats['cache_hits']}")
        print(f"  APIè°ƒç”¨: {self.stats['api_calls']}")
        print(f"  å¤±è´¥è¯·æ±‚: {self.stats['failed_requests']}")
        if self.stats['total_requests'] > 0:
            print(f"  ç¼“å­˜å‘½ä¸­ç‡: {self.stats['cache_hits']/self.stats['total_requests']*100:.1f}%")

    def translate_article(self, article_path):
        """ç¿»è¯‘æ•´ç¯‡æ–‡ç« """
        print(f"æ­£åœ¨ç¿»è¯‘: {article_path}")

        # è¯»å–æ–‡ç« 
        with open(article_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # æå–æ ‡é¢˜
        title_match = re.match(r'^#\s+(.+)$', content, re.MULTILINE)
        if title_match:
            title = title_match.group(1)
        else:
            title = Path(article_path).stem

        print(f"  åŸæ ‡é¢˜: {title}")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»ç¿»è¯‘è¿‡ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰- ä½¿ç”¨åŸæ–‡ä»¶å
        output_dir = Path("pg_essays_cn")
        original_filename = Path(article_path).name
        output_file = output_dir / original_filename
        if output_file.exists():
            print(f"  â­ï¸  è·³è¿‡ï¼ˆå·²å­˜åœ¨ç¿»è¯‘ï¼‰: {original_filename}")
            return None

        # ç¿»è¯‘æ ‡é¢˜
        print("  æ­£åœ¨ç¿»è¯‘æ ‡é¢˜...")
        chinese_title = self.translate_text(title)
        if not chinese_title:
            print("  æ ‡é¢˜ç¿»è¯‘å¤±è´¥")
            return None

        print(f"  ä¸­æ–‡æ ‡é¢˜: {chinese_title}")

        # ç§»é™¤åŸæ ‡é¢˜è¡Œï¼Œå‡†å¤‡ç¿»è¯‘æ­£æ–‡
        content_without_title = re.sub(r'^#\s+.+\n+', '', content, count=1)

        # åˆ†å‰²æ–‡ç« 
        chunks = self.split_article(content_without_title)
        print(f"  æ–‡ç« åˆ†ä¸º {len(chunks)} ä¸ªéƒ¨åˆ†")

        # ç¿»è¯‘æ¯ä¸ªéƒ¨åˆ†
        translated_chunks = []
        for i, chunk in enumerate(chunks):
            print(f"    ç¿»è¯‘ç¬¬ {i + 1}/{len(chunks)} éƒ¨åˆ†...")

            context = f"è¿™æ˜¯Paul Grahamçš„æ–‡ç« ã€Š{title}ã€‹çš„ç¬¬{i + 1}éƒ¨åˆ†ï¼Œå…±{len(chunks)}éƒ¨åˆ†ã€‚"
            translated_chunk = self.translate_text(chunk, context)

            if translated_chunk:
                translated_chunks.append(translated_chunk)
            else:
                print(f"    ç¬¬ {i + 1} éƒ¨åˆ†ç¿»è¯‘å¤±è´¥")
                translated_chunks.append(chunk)  # ä¿ç•™åŸæ–‡

            # æ™ºèƒ½å»¶è¿Ÿ - æ ¹æ®APIé™åˆ¶ä¼˜åŒ– (RPM:1000 â‰ˆ æ¯ç§’16.7æ¬¡)
            # ç¼“å­˜å‘½ä¸­æ—¶ä¸éœ€è¦å»¶è¿Ÿï¼ŒAPIè°ƒç”¨æ—¶é€‚å½“å»¶è¿Ÿ
            if translated_chunk and not self.get_cached_translation(chunk):
                time.sleep(0.1)  # å‡å°‘å»¶è¿Ÿï¼Œæé«˜æ•ˆç‡

        # ç»„åˆç¿»è¯‘ç»“æœ
        translated_content = '\n\n'.join(translated_chunks)

        # æ„å»ºæœ€ç»ˆæ–‡æ¡£ - æ·»åŠ metadataå¤´éƒ¨
        final_content = f"""---
title: "{chinese_title}"
original_title: "{title}"
author: "Paul Graham"
translator: "{self.model} (SiliconFlow)"
translate_date: "{datetime.now().strftime('%Y-%m-%d')}"
source_file: "{Path(article_path).name}"
---

# {chinese_title}

{translated_content}"""

        return {
            'title': chinese_title,
            'original_title': title,
            'content': final_content,
            'model': self.model,
            'filename': Path(article_path).name  # ä½¿ç”¨åŸæ–‡ä»¶å
        }


def clean_filename(filename):
    """æ¸…ç†æ–‡ä»¶å"""
    return re.sub(r'[<>:"/\\|?*]', '_', filename)


def translate_articles():
    """ç¿»è¯‘æ–‡ç« """
    # APIé…ç½®
    api_key = os.getenv('SILICONFLOW_API_KEY')
    if not api_key:
        print("é”™è¯¯ï¼šæœªæ‰¾åˆ°APIå¯†é’¥ï¼")
        print("è¯·ç¡®ä¿åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®äº†SILICONFLOW_API_KEY")
        return

    translator = DeepSeekTranslator(api_key)

    # è¾“å…¥å’Œè¾“å‡ºç›®å½•
    input_dir = Path("pg_essays")
    output_dir = Path("pg_essays_cn")
    output_dir.mkdir(exist_ok=True)

    # è·å–æ‰€æœ‰markdownæ–‡ä»¶
    md_files = list(input_dir.glob("*.md"))
    existing_files = list(output_dir.glob("*.md"))
    print(f"æ‰¾åˆ° {len(md_files)} ç¯‡æ–‡ç« ")
    print(f"å·²ç¿»è¯‘ {len(existing_files)} ç¯‡æ–‡ç« ")

    # é€‰æ‹©è¦ç¿»è¯‘çš„æ–‡ç« æ•°é‡
    print("\né€‰æ‹©ç¿»è¯‘æ•°é‡:")
    print("1. ç¿»è¯‘å‰5ç¯‡ï¼ˆæµ‹è¯•ï¼‰")
    print("2. ç¿»è¯‘å‰20ç¯‡")
    print("3. ç¿»è¯‘æŒ‡å®šæ–‡ç« ")
    print("4. ç¿»è¯‘å…¨éƒ¨")
    print("5. ç»§ç»­ç¿»è¯‘ï¼ˆè·³è¿‡å·²ç¿»è¯‘ï¼‰")

    # é»˜è®¤é€‰æ‹©ç¿»è¯‘å‰5ç¯‡ï¼ˆæµ‹è¯•ï¼‰
    choice = '1'
    print(f"\nè‡ªåŠ¨é€‰æ‹©: {choice} - ç¿»è¯‘å‰5ç¯‡ï¼ˆæµ‹è¯•ï¼‰")

    if choice == '1':
        files_to_translate = md_files[:5]
    elif choice == '2':
        files_to_translate = md_files[:20]
    elif choice == '3':
        print("\nå¯ç”¨æ–‡ç« :")
        for i, file in enumerate(md_files[:20]):
            print(f"{i + 1}. {file.stem}")

        try:
            indices = input("\nè¯·è¾“å…¥è¦ç¿»è¯‘çš„æ–‡ç« ç¼–å·ï¼ˆé€—å·åˆ†éš”ï¼‰: ").split(',')
            files_to_translate = [md_files[int(i.strip()) - 1] for i in indices if i.strip().isdigit()]
        except (ValueError, IndexError):
            print("è¾“å…¥é”™è¯¯ï¼Œé»˜è®¤ç¿»è¯‘å‰5ç¯‡")
            files_to_translate = md_files[:5]
    elif choice == '5':
        # è·³è¿‡å·²ç¿»è¯‘çš„æ–‡ç« 
        translated_titles = {f.stem.split('_')[0] for f in existing_files}
        files_to_translate = [f for f in md_files if f.stem not in translated_titles]
        print(f"å‘ç° {len(files_to_translate)} ç¯‡æœªç¿»è¯‘çš„æ–‡ç« ")
    else:
        files_to_translate = md_files

    print(f"\nå°†ç¿»è¯‘ {len(files_to_translate)} ç¯‡æ–‡ç« ")

    if len(files_to_translate) == 0:
        print("æ²¡æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡ç« ï¼")
        return

    # å¹¶å‘ç¿»è¯‘æ–‡ç« 
    results = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘ç¿»è¯‘
    max_workers = min(translator.max_concurrent, len(files_to_translate))
    print(f"ä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘çº¿ç¨‹è¿›è¡Œç¿»è¯‘...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ç¿»è¯‘ä»»åŠ¡
        future_to_file = {
            executor.submit(translator.translate_article, file_path): file_path 
            for file_path in files_to_translate
        }
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for i, future in enumerate(concurrent.futures.as_completed(future_to_file)):
            file_path = future_to_file[future]
            print(f"\n{'=' * 50}")
            print(f"å®Œæˆè¿›åº¦: {i + 1}/{len(files_to_translate)} - {file_path.name}")
            
            try:
                result = future.result()
                if result:
                    # ä¿å­˜ç¿»è¯‘ç»“æœ - ä½¿ç”¨åŸæ–‡ä»¶å
                    output_path = output_dir / result['filename']

                    with open(output_path, 'w', encoding='utf-8') as f:
                        f.write(result['content'])

                    print(f"  âœ“ å·²ä¿å­˜: {result['filename']}")
                    results.append(result)
                else:
                    print(f"  âœ— ç¿»è¯‘å¤±è´¥")

            except Exception as e:
                print(f"  âœ— å¤„ç†å¤±è´¥: {e}")

    # ä¿å­˜ç¿»è¯‘è®°å½•
    summary = {
        'total_files': len(files_to_translate),
        'successful': len(results),
        'model': translator.model,
        'translated_at': datetime.now().isoformat(),
        'stats': translator.stats,
        'results': results
    }

    with open('translation_log.json', 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    translator.print_stats()
    
    print(f"\n{'=' * 50}")
    print(f"ç¿»è¯‘å®Œæˆï¼")
    print(f"æˆåŠŸç¿»è¯‘: {len(results)} ç¯‡")
    print(f"ç¿»è¯‘æ¨¡å‹: {translator.model}")
    print(f"æ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")
    print(f"ç¿»è¯‘è®°å½•: translation_log.json")


if __name__ == "__main__":
    translate_articles()
