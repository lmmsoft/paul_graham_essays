#!/usr/bin/env python3
"""
åŒè¯­ç”µå­ä¹¦ç”Ÿæˆå™¨ - æ”¯æŒEPUBå’ŒPDFæ ¼å¼çš„ä¸­è‹±å¯¹ç…§ç‰ˆæœ¬
"""

import json
import os
import re
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import requests
from bs4 import BeautifulSoup
from ebooklib import epub
from urllib.parse import urljoin

# å°è¯•å¯¼å…¥weasyprintï¼Œå¦‚æœå¤±è´¥åˆ™ç¦ç”¨PDFåŠŸèƒ½
try:
    import weasyprint
    PDF_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Šï¼šæ— æ³•å¯¼å…¥weasyprintï¼ŒPDFåŠŸèƒ½å°†è¢«ç¦ç”¨: {e}")
    PDF_AVAILABLE = False

from ebook_config import (
    EbookConfig, CSS_STYLES, DATE_PATTERNS, DATE_FORMATS, MARKDOWN_PROCESSING
)


class BilingualEbookGenerator:
    """åŒè¯­ç”µå­ä¹¦ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.config = EbookConfig()
        self.english_dir = Path(self.config.ENGLISH_DIR)
        self.chinese_dir = Path(self.config.CHINESE_DIR)
        self.output_dir = Path(self.config.OUTPUT_DIR)
        self.output_dir.mkdir(exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_articles': 0,
            'bilingual_articles': 0,
            'english_only': 0,
            'chinese_only': 0,
            'failed_processing': 0,
            'total_paragraphs': 0,
            'aligned_paragraphs': 0
        }
    
    def load_metadata(self) -> Optional[Dict]:
        """åŠ è½½æ–‡ç« å…ƒæ•°æ®"""
        metadata_file = Path(self.config.METADATA_FILE)
        if not metadata_file.exists():
            print(f"è­¦å‘Šï¼šå…ƒæ•°æ®æ–‡ä»¶ {metadata_file} ä¸å­˜åœ¨")
            return None
        
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if 'scraped_articles' in data:
                    return data['scraped_articles']
                elif 'articles' in data:
                    return data['articles']
                else:
                    return data
        except Exception as e:
            print(f"è¯»å–å…ƒæ•°æ®å¤±è´¥: {e}")
            return None
    
    def find_article_pairs(self) -> List[Dict]:
        """æ‰¾åˆ°è‹±æ–‡å’Œä¸­æ–‡æ–‡ç« çš„å¯¹åº”å…³ç³»"""
        english_files = {f.name: f for f in self.english_dir.glob("*.md")}
        chinese_files = {f.name: f for f in self.chinese_dir.glob("*.md")}
        
        article_pairs = []
        metadata = self.load_metadata()
        
        for filename in english_files.keys():
            if filename in chinese_files:
                pair_info = {
                    'filename': filename,
                    'english_file': english_files[filename],
                    'chinese_file': chinese_files[filename],
                    'metadata': None
                }
                
                # æŸ¥æ‰¾å¯¹åº”çš„å…ƒæ•°æ®
                if metadata:
                    for article in metadata:
                        if (article.get('filepath', '').endswith(filename) or 
                            filename.replace('.md', '') in article.get('title', '')):
                            pair_info['metadata'] = article
                            break
                
                article_pairs.append(pair_info)
        
        self.stats['total_articles'] = len(english_files)
        self.stats['bilingual_articles'] = len(article_pairs)
        self.stats['english_only'] = len(english_files) - len(article_pairs)
        self.stats['chinese_only'] = len(chinese_files) - len(article_pairs)
        
        print(f"æ‰¾åˆ° {len(article_pairs)} ç¯‡åŒè¯­æ–‡ç« ï¼ˆè‹±æ–‡ {len(english_files)} ç¯‡ï¼Œä¸­æ–‡ {len(chinese_files)} ç¯‡ï¼‰")
        
        return article_pairs
    
    def parse_date_from_text(self, text: str) -> Optional[Dict]:
        """ä»æ–‡æœ¬ä¸­è§£ææ—¥æœŸ"""
        for pattern in DATE_PATTERNS:
            match = re.search(pattern, text)
            if match:
                date_str = match.group(1)
                for fmt in DATE_FORMATS:
                    try:
                        parsed_date = datetime.strptime(date_str, fmt)
                        return {
                            'original': date_str,
                            'parsed': parsed_date,
                            'sortable': parsed_date.strftime('%Y-%m-%d')
                        }
                    except ValueError:
                        continue
        return None
    
    def extract_frontmatter(self, content: str) -> Tuple[Dict, str]:
        """æå–frontmatterå…ƒæ•°æ®"""
        frontmatter = {}
        body = content
        
        if content.startswith('---'):
            try:
                parts = content.split('---', 2)
                if len(parts) >= 3:
                    fm_text = parts[1].strip()
                    body = parts[2].strip()
                    
                    for line in fm_text.split('\n'):
                        if ':' in line:
                            key, value = line.split(':', 1)
                            key = key.strip()
                            value = value.strip().strip('"')
                            frontmatter[key] = value
            except Exception as e:
                print(f"è§£æfrontmatterå¤±è´¥: {e}")
        
        return frontmatter, body
    
    def clean_markdown_text(self, text: str) -> str:
        """æ¸…ç†Markdownæ–‡æœ¬"""
        if not text:
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if MARKDOWN_PROCESSING['clean_html']:
            text = re.sub(r'<[^>]+>', '', text)
        
        # å¤„ç†ç‰¹æ®Šæ ‡è®°
        text = re.sub(r'\[ç¿»è¯‘å¤±è´¥ - åŸæ–‡\]', '', text)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()
    
    def split_into_paragraphs(self, content: str) -> List[str]:
        """å°†å†…å®¹åˆ†å‰²æˆæ®µè½"""
        # ç§»é™¤æ ‡é¢˜è¡Œ
        content = re.sub(r'^#\s+.+\n+', '', content, count=1)
        
        # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²æ®µè½
        paragraphs = content.split('\n\n')
        
        # æ¸…ç†å’Œè¿‡æ»¤æ®µè½
        cleaned_paragraphs = []
        for para in paragraphs:
            para = self.clean_markdown_text(para)
            if para and len(para.strip()) > 10:  # å¿½ç•¥è¿‡çŸ­çš„æ®µè½
                
                # å¦‚æœæ®µè½è¿‡é•¿ï¼Œå°è¯•åˆ†å‰²
                if (MARKDOWN_PROCESSING['split_long_paragraphs'] and 
                    len(para) > MARKDOWN_PROCESSING['max_paragraph_length']):
                    
                    # æŒ‰å¥å·åˆ†å‰²é•¿æ®µè½
                    sentences = re.split(r'\.(?=\s+[A-Z])', para)
                    current_para = ""
                    
                    for sentence in sentences:
                        if len(current_para) + len(sentence) < MARKDOWN_PROCESSING['max_paragraph_length']:
                            current_para += sentence + "."
                        else:
                            if current_para:
                                cleaned_paragraphs.append(current_para.strip())
                            current_para = sentence + "."
                    
                    if current_para:
                        cleaned_paragraphs.append(current_para.strip())
                else:
                    cleaned_paragraphs.append(para)
        
        return cleaned_paragraphs
    
    def align_paragraphs(self, english_paras: List[str], chinese_paras: List[str]) -> List[Tuple[str, str]]:
        """å¯¹é½è‹±æ–‡å’Œä¸­æ–‡æ®µè½"""
        aligned = []
        en_len = len(english_paras)
        cn_len = len(chinese_paras)
        
        if en_len == cn_len:
            # æ•°é‡ç›¸ç­‰ï¼Œç›´æ¥é…å¯¹
            for en, cn in zip(english_paras, chinese_paras):
                aligned.append((en, cn))
        elif en_len > cn_len:
            # è‹±æ–‡æ®µè½æ›´å¤šï¼Œä¸­æ–‡æ®µè½å¯èƒ½è¢«åˆå¹¶äº†
            ratio = en_len / cn_len
            for i, cn in enumerate(chinese_paras):
                start_idx = int(i * ratio)
                end_idx = min(int((i + 1) * ratio), en_len)
                combined_en = '\n\n'.join(english_paras[start_idx:end_idx])
                aligned.append((combined_en, cn))
        else:
            # ä¸­æ–‡æ®µè½æ›´å¤šï¼Œè‹±æ–‡æ®µè½å¯èƒ½è¢«åˆå¹¶äº†
            ratio = cn_len / en_len
            for i, en in enumerate(english_paras):
                start_idx = int(i * ratio)
                end_idx = min(int((i + 1) * ratio), cn_len)
                combined_cn = '\n\n'.join(chinese_paras[start_idx:end_idx])
                aligned.append((en, combined_cn))
        
        self.stats['total_paragraphs'] += max(en_len, cn_len)
        self.stats['aligned_paragraphs'] += len(aligned)
        
        return aligned
    
    def process_article_pair(self, pair_info: Dict) -> Optional[Dict]:
        """å¤„ç†ä¸€å¯¹è‹±ä¸­æ–‡ç« """
        try:
            # è¯»å–è‹±æ–‡æ–‡ç« 
            with open(pair_info['english_file'], 'r', encoding='utf-8') as f:
                english_content = f.read()
            
            # è¯»å–ä¸­æ–‡æ–‡ç« 
            with open(pair_info['chinese_file'], 'r', encoding='utf-8') as f:
                chinese_content = f.read()
            
            # æå–å…ƒæ•°æ®
            en_frontmatter, en_body = self.extract_frontmatter(english_content)
            cn_frontmatter, cn_body = self.extract_frontmatter(chinese_content)
            
            # æå–æ ‡é¢˜
            en_title_match = re.match(r'^#\s+(.+)$', en_body, re.MULTILINE)
            cn_title_match = re.match(r'^#\s+(.+)$', cn_body, re.MULTILINE)
            
            english_title = en_title_match.group(1) if en_title_match else pair_info['filename'].replace('.md', '')
            chinese_title = cn_title_match.group(1) if cn_title_match else cn_frontmatter.get('title', english_title)
            
            # æå–å‘å¸ƒæ—¥æœŸ
            date_info = None
            if pair_info['metadata'] and 'date' in pair_info['metadata']:
                if isinstance(pair_info['metadata']['date'], dict):
                    date_info = pair_info['metadata']['date']
                else:
                    date_info = self.parse_date_from_text(str(pair_info['metadata']['date']))
            
            if not date_info:
                # ä»æ–‡ç« å†…å®¹ä¸­æå–æ—¥æœŸ
                date_info = self.parse_date_from_text(en_body)
            
            # æå–URL
            url = ""
            if pair_info['metadata'] and 'url' in pair_info['metadata']:
                url = pair_info['metadata']['url']
            
            # åˆ†å‰²æ®µè½
            english_paragraphs = self.split_into_paragraphs(en_body)
            chinese_paragraphs = self.split_into_paragraphs(cn_body)
            
            # å¯¹é½æ®µè½
            aligned_paragraphs = self.align_paragraphs(english_paragraphs, chinese_paragraphs)
            
            processed_article = {
                'filename': pair_info['filename'],
                'english_title': english_title,
                'chinese_title': chinese_title,
                'date': date_info,
                'url': url,
                'aligned_paragraphs': aligned_paragraphs,
                'frontmatter': {
                    'english': en_frontmatter,
                    'chinese': cn_frontmatter
                }
            }
            
            return processed_article
            
        except Exception as e:
            print(f"å¤„ç†æ–‡ç«  {pair_info['filename']} æ—¶å‡ºé”™: {e}")
            self.stats['failed_processing'] += 1
            return None
    
    def convert_markdown_to_html(self, text: str) -> str:
        """å°†Markdownæ–‡æœ¬è½¬æ¢ä¸ºHTML"""
        # å¤„ç†åŸºæœ¬çš„Markdownè¯­æ³•
        # ç²—ä½“
        text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', text)
        # æ–œä½“
        text = re.sub(r'\*(.*?)\*', r'<em>\1</em>', text)
        # é“¾æ¥
        text = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2">\1</a>', text)
        # ä»£ç 
        text = re.sub(r'`([^`]+)`', r'<code>\1</code>', text)
        
        # å¤„ç†æ®µè½
        paragraphs = text.split('\n\n')
        html_paragraphs = []
        
        for para in paragraphs:
            para = para.strip()
            if para:
                if para.startswith('#'):
                    # æ ‡é¢˜
                    level = len(para) - len(para.lstrip('#'))
                    para = f'<h{min(level, 6)}>{para.lstrip("#").strip()}</h{min(level, 6)}>'
                else:
                    # æ™®é€šæ®µè½
                    para = f'<p>{para}</p>'
                html_paragraphs.append(para)
        
        return '\n'.join(html_paragraphs)
    
    def create_article_html(self, article: Dict) -> str:
        """åˆ›å»ºæ–‡ç« çš„HTMLå†…å®¹"""
        html_parts = []
        
        # æ–‡ç« å¤´éƒ¨ä¿¡æ¯
        html_parts.append('<div class="article-header">')
        html_parts.append(f'<div class="article-title">')
        html_parts.append(f'  <div>{article["english_title"]}</div>')
        html_parts.append(f'  <div>{article["chinese_title"]}</div>')
        html_parts.append(f'</div>')
        
        if article.get('date'):
            html_parts.append(f'<div class="article-date">{article["date"]["original"]}</div>')
        
        if article.get('url'):
            html_parts.append(f'<div class="article-url"><a href="{article["url"]}">{article["url"]}</a></div>')
        
        html_parts.append('</div>')
        
        # åŒè¯­å¯¹ç…§å†…å®¹
        for en_para, cn_para in article['aligned_paragraphs']:
            html_parts.append('<div class="bilingual-section">')
            
            # è‹±æ–‡æ®µè½
            html_parts.append('<div class="english-paragraph">')
            en_html = self.convert_markdown_to_html(en_para)
            html_parts.append(en_html)
            html_parts.append('</div>')
            
            # ä¸­æ–‡æ®µè½
            is_failed = '[ç¿»è¯‘å¤±è´¥ - åŸæ–‡]' in cn_para
            css_class = 'chinese-paragraph failed-translation' if is_failed else 'chinese-paragraph'
            
            html_parts.append(f'<div class="{css_class}">')
            cn_html = self.convert_markdown_to_html(cn_para)
            html_parts.append(cn_html)
            html_parts.append('</div>')
            
            html_parts.append('</div>')
        
        return '\n'.join(html_parts)
    
    def create_epub(self, articles: List[Dict], output_path: Path) -> bool:
        """åˆ›å»ºEPUBç”µå­ä¹¦"""
        try:
            book = epub.EpubBook()
            book.set_identifier(str(uuid.uuid4()))
            book.set_title("Paul Graham Essays - Bilingual Collection")
            book.set_language('en')
            book.add_author(self.config.AUTHOR)
            
            # æ·»åŠ å…ƒæ•°æ®
            book.add_metadata('DC', 'description', 
                f'Bilingual collection of {len(articles)} essays by Paul Graham (English & Chinese)')
            book.add_metadata('DC', 'date', datetime.now().strftime('%Y-%m-%d'))
            book.add_metadata('DC', 'subject', 'Technology, Startups, Programming')
            
            # åˆ›å»ºCSSæ ·å¼
            nav_css = epub.EpubItem(
                uid="style_nav",
                file_name="style.css",
                media_type="text/css",
                content=CSS_STYLES['epub']
            )
            book.add_item(nav_css)
            
            # åˆ›å»ºå°é¢
            cover_html = epub.EpubHtml(
                title='Cover',
                file_name='cover.xhtml',
                lang='en'
            )
            
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            dated_articles = [a for a in articles if a.get('date')]
            if dated_articles:
                dates = [a['date']['parsed'] for a in dated_articles]
                min_date = min(dates).strftime('%B %Y')
                max_date = max(dates).strftime('%B %Y')
                date_range = f"{min_date} - {max_date}"
            else:
                date_range = "Various dates"
            
            cover_html.content = f'''
            <html>
            <head>
                <link rel="stylesheet" type="text/css" href="style.css"/>
            </head>
            <body>
                <div class="article-header">
                    <h1>Paul Graham Essays</h1>
                    <h2>Bilingual Collection<br/>åŒè¯­åˆé›†</h2>
                    <p>A collection of {len(articles)} essays in English and Chinese</p>
                    <p>åŒ…å« {len(articles)} ç¯‡ä¸­è‹±å¯¹ç…§æ–‡ç« </p>
                    <p>Date range: {date_range}</p>
                    <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                    <hr/>
                    <p><small>This ebook was created for personal reading.<br/>
                    All content Â© Paul Graham. Chinese translations by AI.</small></p>
                </div>
            </body>
            </html>
            '''
            book.add_item(cover_html)
            
            # æŒ‰æ—¥æœŸæ’åºæ–‡ç« ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            sorted_articles = sorted(
                articles,
                key=lambda x: x['date']['parsed'] if x.get('date') else datetime(1900, 1, 1),
                reverse=True
            )
            
            # åˆ›å»ºç›®å½•é¡µ
            toc_html = epub.EpubHtml(
                title='Table of Contents',
                file_name='toc.xhtml',
                lang='en'
            )
            
            toc_content = f'''
            <html>
            <head>
                <link rel="stylesheet" type="text/css" href="style.css"/>
            </head>
            <body>
                <div class="toc">
                    <h2>Table of Contents / ç›®å½•</h2>
                    <ol>
            '''
            
            for i, article in enumerate(sorted_articles):
                toc_content += f'''
                    <li>
                        <a href="essay_{i+1}.xhtml">
                            {article["english_title"]}<br/>
                            <small>{article["chinese_title"]}</small>
                        </a>
                '''
                if article.get('date'):
                    toc_content += f'<span class="date">({article["date"]["original"]})</span>'
                toc_content += '</li>'
            
            toc_content += '''
                    </ol>
                </div>
            </body>
            </html>
            '''
            
            toc_html.content = toc_content
            book.add_item(toc_html)
            
            # åˆ›å»ºæ–‡ç« ç« èŠ‚
            chapters = [cover_html, toc_html]
            
            for i, article in enumerate(sorted_articles):
                chapter = epub.EpubHtml(
                    title=f"{article['english_title']} / {article['chinese_title']}",
                    file_name=f'essay_{i+1}.xhtml',
                    lang='en'
                )
                
                article_html = self.create_article_html(article)
                
                chapter.content = f'''
                <html>
                <head>
                    <link rel="stylesheet" type="text/css" href="style.css"/>
                    <title>{article["english_title"]}</title>
                </head>
                <body>
                    <div class="page-break"></div>
                    {article_html}
                </body>
                </html>
                '''
                
                # æ·»åŠ æ ·å¼å¼•ç”¨
                chapter.add_item(nav_css)
                book.add_item(chapter)
                chapters.append(chapter)
            
            # è®¾ç½®ç›®å½•å’Œä¹¦è„Š
            book.toc = chapters[2:]  # ä¸åŒ…æ‹¬å°é¢å’Œç›®å½•é¡µ
            book.spine = chapters
            
            # æ·»åŠ å¯¼èˆª
            book.add_item(epub.EpubNcx())
            book.add_item(epub.EpubNav())
            
            # å†™å…¥EPUBæ–‡ä»¶
            epub.write_epub(str(output_path), book)
            print(f"âœ“ EPUBå·²åˆ›å»º: {output_path}")
            print(f"  æ–‡ä»¶å¤§å°: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB")
            
            return True
            
        except Exception as e:
            print(f"åˆ›å»ºEPUBå¤±è´¥: {e}")
            return False
    
    def create_pdf(self, articles: List[Dict], output_path: Path) -> bool:
        """åˆ›å»ºPDFç”µå­ä¹¦"""
        if not PDF_AVAILABLE:
            print("âŒ PDFåŠŸèƒ½ä¸å¯ç”¨ï¼šweasyprintæœªæ­£ç¡®å®‰è£…")
            return False
        
        try:
            # æŒ‰æ—¥æœŸæ’åºæ–‡ç« ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            sorted_articles = sorted(
                articles,
                key=lambda x: x['date']['parsed'] if x.get('date') else datetime(1900, 1, 1),
                reverse=True
            )
            
            # åˆ›å»ºHTMLå†…å®¹
            html_parts = []
            
            # HTMLå¤´éƒ¨
            html_parts.append(f'''
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Paul Graham Essays - Bilingual Collection</title>
                <style>
                {CSS_STYLES['pdf']}
                </style>
            </head>
            <body>
            ''')
            
            # å°é¢é¡µ
            dated_articles = [a for a in articles if a.get('date')]
            if dated_articles:
                dates = [a['date']['parsed'] for a in dated_articles]
                min_date = min(dates).strftime('%B %Y')
                max_date = max(dates).strftime('%B %Y')
                date_range = f"{min_date} - {max_date}"
            else:
                date_range = "Various dates"
            
            html_parts.append(f'''
            <div class="article-header">
                <h1>Paul Graham Essays</h1>
                <h2>Bilingual Collection / åŒè¯­åˆé›†</h2>
                <p>A collection of {len(articles)} essays in English and Chinese</p>
                <p>åŒ…å« {len(articles)} ç¯‡ä¸­è‹±å¯¹ç…§æ–‡ç« </p>
                <p>Date range: {date_range}</p>
                <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                <hr/>
                <p><small>This book was created for personal reading.<br/>
                All content Â© Paul Graham. Chinese translations by AI.</small></p>
            </div>
            ''')
            
            # ç›®å½•é¡µ
            html_parts.append('''
            <div class="page-break"></div>
            <div class="toc">
                <h2>Table of Contents / ç›®å½•</h2>
                <ol>
            ''')
            
            for i, article in enumerate(sorted_articles):
                html_parts.append(f'''
                <li>
                    {article["english_title"]}<br/>
                    <small>{article["chinese_title"]}</small>
                ''')
                if article.get('date'):
                    html_parts.append(f'<span class="date">({article["date"]["original"]})</span>')
                html_parts.append('</li>')
            
            html_parts.append('''
                </ol>
            </div>
            ''')
            
            # æ–‡ç« å†…å®¹
            for article in sorted_articles:
                html_parts.append('<div class="page-break"></div>')
                article_html = self.create_article_html(article)
                html_parts.append(article_html)
            
            # HTMLç»“å°¾
            html_parts.append('''
            </body>
            </html>
            ''')
            
            # åˆå¹¶HTML
            full_html = '\n'.join(html_parts)
            
            # ç”ŸæˆPDF
            print(f"æ­£åœ¨ç”ŸæˆPDF: {output_path}")
            weasyprint.HTML(string=full_html, base_url=str(self.output_dir)).write_pdf(str(output_path))
            
            print(f"âœ“ PDFå·²åˆ›å»º: {output_path}")
            print(f"  æ–‡ä»¶å¤§å°: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB")
            
            return True
            
        except Exception as e:
            print(f"åˆ›å»ºPDFå¤±è´¥: {e}")
            return False
    
    def create_html(self, articles: List[Dict], output_path: Path) -> bool:
        """åˆ›å»ºHTMLç‰ˆæœ¬"""
        try:
            # æŒ‰æ—¥æœŸæ’åºæ–‡ç« ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            sorted_articles = sorted(
                articles,
                key=lambda x: x['date']['parsed'] if x.get('date') else datetime(1900, 1, 1),
                reverse=True
            )
            
            # åˆ›å»ºHTMLå†…å®¹
            html_parts = []
            
            # HTMLå¤´éƒ¨
            html_parts.append(f'''
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Paul Graham Essays - Bilingual Collection</title>
                <style>
                {CSS_STYLES['html']}
                </style>
            </head>
            <body>
            ''')
            
            # å°é¢
            dated_articles = [a for a in articles if a.get('date')]
            if dated_articles:
                dates = [a['date']['parsed'] for a in dated_articles]
                min_date = min(dates).strftime('%B %Y')
                max_date = max(dates).strftime('%B %Y')
                date_range = f"{min_date} - {max_date}"
            else:
                date_range = "Various dates"
            
            html_parts.append(f'''
            <div class="article-header">
                <h1>Paul Graham Essays</h1>
                <h2>Bilingual Collection / åŒè¯­åˆé›†</h2>
                <p>A collection of {len(articles)} essays in English and Chinese</p>
                <p>åŒ…å« {len(articles)} ç¯‡ä¸­è‹±å¯¹ç…§æ–‡ç« </p>
                <p>Date range: {date_range}</p>
                <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                <hr/>
                <p><small>This webpage was created for personal reading.<br/>
                All content Â© Paul Graham. Chinese translations by AI.</small></p>
            </div>
            ''')
            
            # ç›®å½•
            html_parts.append('''
            <div class="toc">
                <h2>Table of Contents / ç›®å½•</h2>
                <ol>
            ''')
            
            for i, article in enumerate(sorted_articles):
                article_id = f"article_{i+1}"
                html_parts.append(f'''
                <li>
                    <a href="#{article_id}">
                        {article["english_title"]}<br/>
                        <small>{article["chinese_title"]}</small>
                    </a>
                ''')
                if article.get('date'):
                    html_parts.append(f'<span class="date">({article["date"]["original"]})</span>')
                html_parts.append('</li>')
            
            html_parts.append('''
                </ol>
            </div>
            ''')
            
            # æ–‡ç« å†…å®¹
            for i, article in enumerate(sorted_articles):
                article_id = f"article_{i+1}"
                html_parts.append(f'<div id="{article_id}">')
                article_html = self.create_article_html(article)
                html_parts.append(article_html)
                html_parts.append('</div>')
                html_parts.append('<hr/>')
            
            # HTMLç»“å°¾
            html_parts.append('''
            </body>
            </html>
            ''')
            
            # ä¿å­˜HTMLæ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(html_parts))
            
            print(f"âœ“ HTMLå·²åˆ›å»º: {output_path}")
            print(f"  æ–‡ä»¶å¤§å°: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB")
            
            return True
            
        except Exception as e:
            print(f"åˆ›å»ºHTMLå¤±è´¥: {e}")
            return False
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
        print(f"  æ€»æ–‡ç« æ•°: {self.stats['total_articles']}")
        print(f"  åŒè¯­æ–‡ç« : {self.stats['bilingual_articles']}")
        print(f"  ä»…è‹±æ–‡: {self.stats['english_only']}")
        print(f"  ä»…ä¸­æ–‡: {self.stats['chinese_only']}")
        print(f"  å¤„ç†å¤±è´¥: {self.stats['failed_processing']}")
        print(f"  æ€»æ®µè½æ•°: {self.stats['total_paragraphs']}")
        print(f"  å¯¹é½æ®µè½: {self.stats['aligned_paragraphs']}")
        if self.stats['total_paragraphs'] > 0:
            alignment_rate = self.stats['aligned_paragraphs'] / self.stats['total_paragraphs'] * 100
            print(f"  å¯¹é½æˆåŠŸç‡: {alignment_rate:.1f}%")
    
    def generate_ebooks(self, formats: List[str] = None) -> Dict[str, bool]:
        """ç”Ÿæˆç”µå­ä¹¦"""
        if formats is None:
            formats = ['epub', 'pdf', 'html']
        
        print("ğŸ” æ­£åœ¨æ‰«ææ–‡ç« ...")
        article_pairs = self.find_article_pairs()
        
        if not article_pairs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒè¯­æ–‡ç« ï¼")
            return {}
        
        print(f"\nğŸ“ æ­£åœ¨å¤„ç† {len(article_pairs)} ç¯‡æ–‡ç« ...")
        processed_articles = []
        
        for i, pair in enumerate(article_pairs):
            print(f"  å¤„ç†è¿›åº¦: {i+1}/{len(article_pairs)} - {pair['filename']}")
            article = self.process_article_pair(pair)
            if article:
                processed_articles.append(article)
        
        if not processed_articles:
            print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ç« ï¼")
            return {}
        
        print(f"\nâœ… æˆåŠŸå¤„ç† {len(processed_articles)} ç¯‡æ–‡ç« ")
        
        # è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_files = self.config.get_output_files()
        results = {}
        
        # ç”Ÿæˆå„ç§æ ¼å¼
        print(f"\nğŸ“š æ­£åœ¨ç”Ÿæˆç”µå­ä¹¦...")
        
        if 'epub' in formats:
            print("\nğŸ“– ç”ŸæˆEPUB...")
            results['epub'] = self.create_epub(processed_articles, output_files['epub'])
        
        if 'pdf' in formats:
            print("\nğŸ“„ ç”ŸæˆPDF...")
            results['pdf'] = self.create_pdf(processed_articles, output_files['pdf'])
        
        if 'html' in formats:
            print("\nğŸŒ ç”ŸæˆHTML...")
            results['html'] = self.create_html(processed_articles, output_files['html'])
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_stats()
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Paul Graham Essays - åŒè¯­ç”µå­ä¹¦ç”Ÿæˆå™¨")
    print("=" * 50)
    
    generator = BilingualEbookGenerator()
    
    # è®©ç”¨æˆ·é€‰æ‹©æ ¼å¼
    print("\né€‰æ‹©è¦ç”Ÿæˆçš„æ ¼å¼:")
    print("1. ä»…EPUB")
    if PDF_AVAILABLE:
        print("2. ä»…PDF")
    else:
        print("2. ä»…PDF (ä¸å¯ç”¨)")
    print("3. ä»…HTML")
    if PDF_AVAILABLE:
        print("4. EPUB + PDF")
        print("5. å…¨éƒ¨æ ¼å¼ (EPUB + PDF + HTML)")
    else:
        print("4. EPUB + HTML")
    
    choice = input(f"\nè¯·é€‰æ‹© (1-{'5' if PDF_AVAILABLE else '4'}): ").strip()
    
    if PDF_AVAILABLE:
        format_map = {
            '1': ['epub'],
            '2': ['pdf'],
            '3': ['html'],
            '4': ['epub', 'pdf'],
            '5': ['epub', 'pdf', 'html']
        }
    else:
        format_map = {
            '1': ['epub'],
            '2': ['html'],  # å¦‚æœPDFä¸å¯ç”¨ï¼Œé€‰é¡¹2å˜æˆHTML
            '3': ['html'],
            '4': ['epub', 'html']
        }
    
    formats = format_map.get(choice, ['epub', 'pdf', 'html'])
    
    print(f"\nå°†ç”Ÿæˆæ ¼å¼: {', '.join(formats).upper()}")
    
    # ç”Ÿæˆç”µå­ä¹¦
    results = generator.generate_ebooks(formats)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\n{'=' * 50}")
    print("ğŸ“š ç”Ÿæˆå®Œæˆï¼")
    
    success_count = sum(1 for success in results.values() if success)
    total_count = len(results)
    
    print(f"æˆåŠŸç”Ÿæˆ: {success_count}/{total_count} ç§æ ¼å¼")
    
    for format_name, success in results.items():
        status = "âœ…" if success else "âŒ"
        print(f"  {status} {format_name.upper()}")
    
    if success_count > 0:
        print(f"\næ–‡ä»¶ä¿å­˜åœ¨: {generator.output_dir}")


if __name__ == "__main__":
    main()